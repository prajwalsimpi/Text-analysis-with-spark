# -*- coding: utf-8 -*-
"""bigdataproject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cFku45TAsoqc8O-fQKOG9kj3aZVzASYG
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://mirrors.estointernet.in/apache/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz
!tar xf spark-2.4.7-bin-hadoop2.7.tgz
!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.7-bin-hadoop2.7"

import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()

import pandas as pd

import numpy as np
import nltk
import os

"""Getting data, tokenizing, finding the frequency distribution"""

import nltk.corpus  
with open('/content/sample_data/reviews.txt', 'r') as file:
    data = file.read().replace('\n', '')                                                              # sample text for performing tokenization
 # importing word_tokenize from nltk
from nltk.tokenize import word_tokenize
from nltk.tokenize import RegexpTokenizer

tokenizer = RegexpTokenizer(r'\w+')
token = tokenizer.tokenize(data)              # Passing the string text into word tokenize for breaking the sentences
#token

"""Frequency Distribution 

"""

# finding the frequency distinct in the tokens
# Importing FreqDist library from nltk and passing token into FreqDist
from nltk.probability import FreqDist
fdist = FreqDist(token)
fdist

"""Removing Stop Words

"""

#Stop words
# importing stopwors from nltk library
import nltk
nltk.download('stopwords')
from nltk import word_tokenize
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
print(lemmatizedList)
stop_words.add('It')
stop_words.add('wa')
stop_words.add('I')
stop_words.add('br')
print(len(lemmatizedList))
stopwordRemoved = [x for x in lemmatizedList if x not in stop_words]
print(stopwordRemoved)
print(len(stopwordRemoved))

"""Steming using PorterStemmer"""

from nltk.stem import PorterStemmer
pst = PorterStemmer()
stemmedList = []
for word in token:
  stemmOutput = pst.stem(word)
  stemmedList.append(stemmOutput)
stemmedList
print(len(stemmedList))

"""Lemmatization"""

# Importing Lemmatizer library from nltk
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer() 
nltk.download('wordnet')
lemmatizedList = []
for word in stemmedList:
  lemmatizedOutput = lemmatizer.lemmatize(word)
  lemmatizedList.append(lemmatizedOutput)
print(lemmatizedList)
print(len(lemmatizedList))

"""Output Representation"""

#most 20 frequent words in a list of tuples
from nltk.probability import FreqDist
fdist = FreqDist(stopwordRemoved)
mostFreq =fdist.most_common(20) 
print(mostFreq)

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline

pd.DataFrame(mostFreq, columns=['word','frequency']).set_index('word').plot(kind='bar');